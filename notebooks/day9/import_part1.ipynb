{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sadsadasfwearvsrser\n",
      "srf\n",
      "scdtg\n",
      "bfh\n",
      "rb\n",
      "yh\n",
      "rtdy\n",
      "tryrtytueitenuhbrtfdyvge\n",
      "vtw\n",
      "tv\n",
      "tybtrurgunnyjyun5e\n",
      "byer\n",
      "yhrtbytyfjhjtynuymih\n",
      "5ndry\n",
      "t4ebs\n",
      "hrtdube5runjyuiytmun\n",
      "rtubje\n",
      "tdutunjtrimjyuktuyiklip,ormf du rdtyvyvbtryvr\n",
      "\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Open a file: file\n",
    "file = open(\"moby_dick.txt\", \"r\")\n",
    "\n",
    "# Print it\n",
    "print(file.read())\n",
    "\n",
    "# Check whether file is closed\n",
    "print(file.closed)\n",
    "\n",
    "# Close file\n",
    "file.close()\n",
    "\n",
    "# Check whether file is closed\n",
    "print(file.closed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sadsadasfwearvsrser\n",
      "\n",
      "srf\n",
      "\n",
      "scdtg\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read & print the first 3 lines\n",
    "with open('moby_dick.txt') as file:\n",
    "    print(file.readline())\n",
    "    print(file.readline())\n",
    "    print(file.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import package\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Assign filename to variable: file\n",
    "file = 'digits.csv'\n",
    "\n",
    "# Load file as array: digits\n",
    "digits = np.loadtxt(\"digits.csv\", delimiter=\",\")\n",
    "\n",
    "# Print datatype of digits\n",
    "print(type(digits))\n",
    "\n",
    "# Select and reshape a row\n",
    "im = digits[21, 1:]\n",
    "im_sq = np.reshape(im, (28, 28))\n",
    "\n",
    "# Plot reshaped data (matplotlib.pyplot already loaded as plt)\n",
    "plt.imshow(im_sq, cmap='Greys', interpolation='nearest')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the arguments of np.loadtxt(): the file you're importing is tab-delimited, \n",
    "\n",
    "you want to skip the first row and you only want to import the first and third columns.\n",
    "\n",
    "Complete the argument of the print() call in order to print the entire array that you just imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import numpy\n",
    "import numpy as np\n",
    "\n",
    "# Assign the filename: file\n",
    "file = 'digits_header.txt'\n",
    "\n",
    "# Load the data: data\n",
    "data = np.loadtxt(file, delimiter=\"\\t\", skiprows=1, usecols=[0,2])\n",
    "\n",
    "# Print data\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the first call to np.loadtxt() by passing file as the first argument.\n",
    "\n",
    "Execute print(data[0]) to print the first element of data.\n",
    "\n",
    "Complete the second call to np.loadtxt(). The file you're importing is tab-delimited, the datatype is float, and \n",
    "you want to skip the first row.\n",
    "\n",
    "Print the 10th element of data_float by completing the print() command. Be guided by the previous print() call.\n",
    "\n",
    "Execute the rest of the code to visualize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign filename: file\n",
    "file = 'seaslug.txt'\n",
    "\n",
    "# Import file: data\n",
    "data = np.loadtxt(file, delimiter='\\t', dtype=str)\n",
    "\n",
    "# Print the first element of data\n",
    "print(data[0])\n",
    "\n",
    "# Import data as floats and skip the first row: data_float\n",
    "data_float = np.loadtxt(file, delimiter='\\t', dtype=float, skiprows=1)\n",
    "\n",
    "# Print the 10th element of data_float\n",
    "print(data_float[9])\n",
    "\n",
    "# Plot a scatterplot of the data\n",
    "plt.scatter(data_float[:, 0], data_float[:, 1])\n",
    "plt.xlabel('time (min.)')\n",
    "plt.ylabel('percentage of larvae')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the sep (the pandas version of delim), comment and na_values arguments of pd.read_csv(). \n",
    "\n",
    "comment takes characters that comments occur after in the file, which in this case is '#'. \n",
    "\n",
    "na_values takes a list of strings to recognize as NA/NaN, in this case the string 'Nothing'.\n",
    "\n",
    "Execute the rest of the code to print the head of the resulting DataFrame and \n",
    "\n",
    "plot the histogram of the 'Age' of passengers aboard the Titanic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assign filename: file\n",
    "file = 'titanic_corrupt.txt'\n",
    "\n",
    "# Import file: data\n",
    "data = pd.read_csv(file, sep=\"\\t\", comment=\"#\", na_values=\"Nothing\")\n",
    "\n",
    "# Print the head of the DataFrame\n",
    "print(data.head())\n",
    "\n",
    "# Plot 'Age' variable in a histogram\n",
    "pd.DataFrame.hist(data[['Age']])\n",
    "plt.xlabel('Age (years)')\n",
    "plt.ylabel('count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the second argument of open() so that it is read only for a binary file. This argument will be a string of two letters, one signifying 'read only', the other 'binary'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pickle package\n",
    "import pickle\n",
    "\n",
    "# Open pickle file and load data: d\n",
    "with open('data.pkl', \"rb\") as file:\n",
    "    d = pickle.load(file)\n",
    "\n",
    "# Print d\n",
    "print(d)\n",
    "\n",
    "# Print datatype of d\n",
    "print(type(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Assign spreadsheet filename: file\n",
    "file = \"battledeath.xlsx\"\n",
    "\n",
    "# Load spreadsheet: xl\n",
    "xls = pd.ExcelFile(file)\n",
    "\n",
    "# Print sheet names\n",
    "print(xls.sheet_names)\n",
    "\n",
    "\n",
    ">>> ['2002', '2004']\n",
    "\n",
    "\n",
    "# Load a sheet into a DataFrame by name: df1\n",
    "df1 = xls.parse(\"2004\")\n",
    "\n",
    "# Print the head of the DataFrame df1\n",
    "print(df1.head())\n",
    "\n",
    "# Load a sheet into a DataFrame by index: df2\n",
    "df2 = xls.parse(0)\n",
    "\n",
    "# Print the head of the DataFrame df2\n",
    "print(df2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse the first sheet by index. In doing so, skip the first row of data and name the columns 'Country' and 'AAM due to War (2002)' using the argument names. \n",
    "\n",
    "The values passed to skiprows and names all need to be of type list.\n",
    "\n",
    "Parse the second sheet by index. In doing so, parse only the first column with the usecols parameter, skip the first row and rename the column 'Country'.\n",
    "\n",
    "The argument passed to usecols also needs to be of type list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the first sheet and rename the columns: df1\n",
    "df1 = xls.parse(0, skiprows=1, names=['Country','AAM due to War (2002)'])\n",
    "\n",
    "# Print the head of the DataFrame df1\n",
    "print(df1.head())\n",
    "\n",
    "# Parse the first column of the second sheet and rename the column: df2\n",
    "df2 = xls.parse(1, usecols=[0], skiprows=[0], names=[\"Country\"])\n",
    "\n",
    "# Print the head of the DataFrame df2\n",
    "print(df2.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAS file, Statistical Analysis System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the module SAS7BDAT from the library sas7bdat.\n",
    "\n",
    "In the context of the file 'sales.sas7bdat', load its contents to a DataFrame df_sas, using the method \n",
    "to_data_frame() on the object file.\n",
    "\n",
    "Print the head of the DataFrame df_sas.\n",
    "\n",
    "Execute your entire script to produce a histogram plot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import sas7bdat package\n",
    "from sas7bdat import SAS7BDAT\n",
    "\n",
    "# Save file to a DataFrame: df_sas\n",
    "with SAS7BDAT('sales.sas7bdat') as file:\n",
    "    df_sas = file.to_data_frame()\n",
    "\n",
    "# Print head of DataFrame\n",
    "print(df_sas.head())\n",
    "\n",
    "# Plot histogram of DataFrame features (pandas and pyplot already imported)\n",
    "pd.DataFrame.hist(df_sas[['P']])\n",
    "plt.ylabel('count')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#df = pd.read_stata('disarea.dta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDF5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the package h5py.\n",
    "\n",
    "Assign the name of the file to the variable file.\n",
    "\n",
    "Load the file as read only into the variable data.\n",
    "\n",
    "Print the names of the groups in the HDF5 file 'LIGO_data.hdf5'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "# Assign filename: file\n",
    "file = \"LIGO_data.hdf5\"\n",
    "\n",
    "# Load file: data\n",
    "data = h5py.File(file, \"r\")\n",
    "\n",
    "# Print the datatype of the loaded file\n",
    "print(type(data))\n",
    "\n",
    "# Print the keys of the file\n",
    "for key in data['meta'].keys():\n",
    "    print(key)\n",
    "    \n",
    "# It has generally three things :\n",
    "# meta\n",
    "# strain\n",
    "# quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign the HDF5 group data['strain'] to group.\n",
    "\n",
    "In the for loop, print out the keys of the HDF5 group in group.\n",
    "\n",
    "Assign to the variable strain the values of the time series data data['strain']['Strain'] using the attribute .value.\n",
    "\n",
    "Set num_samples equal to 10000, the number of time points we wish to sample.\n",
    "\n",
    "Execute the rest of the code to produce a plot of the time series data in LIGO_data.hdf5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the HDF5 group: group\n",
    "group = data['strain']\n",
    "\n",
    "# Check out keys of group\n",
    "for key in group.keys():\n",
    "    print(key)\n",
    "\n",
    "# Set variable equal to time series data: strain\n",
    "strain = data['strain']['Strain'].value\n",
    "\n",
    "# Set number of time points to sample: num_samples\n",
    "num_samples = 10000\n",
    "\n",
    "# Set time vector\n",
    "time = np.arange(0, 1, 1/num_samples)\n",
    "\n",
    "# Plot data\n",
    "plt.plot(time, strain[:num_samples])\n",
    "plt.xlabel('GPS Time (s)')\n",
    "plt.ylabel('strain')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loaded with scipy\n",
    "# Import package\n",
    "import scipy.io\n",
    "\n",
    "# Load MATLAB file: mat\n",
    "mat = scipy.io.loadmat(\"albeck_gene_expression.mat\")\n",
    "\n",
    "# Print the datatype type of mat\n",
    "print(type(mat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the method .keys() on the dictionary mat to print the keys.\n",
    "\n",
    "Most of these keys (in fact the ones that do NOT begin and end with '__') are variables from the corresponding MATLAB environment.\n",
    "\n",
    "Print the type of the value corresponding to the key 'CYratioCyt' in mat. \n",
    "\n",
    "Recall that mat['CYratioCyt'] accesses the value.\n",
    "\n",
    "Print the shape of the value corresponding to the key 'CYratioCyt' using the numpy function shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the keys of the MATLAB dictionary\n",
    "print(mat.keys())\n",
    "\n",
    "# Print the type of the value corresponding to the key 'CYratioCyt'\n",
    "print(type(mat['CYratioCyt']))\n",
    "\n",
    "# Print the shape of the value corresponding to the key 'CYratioCyt'\n",
    "print(mat['CYratioCyt'].shape)\n",
    "\n",
    "# Subset the array and plot it\n",
    "data = mat['CYratioCyt'][25, 5:]\n",
    "fig = plt.figure()\n",
    "plt.plot(data)\n",
    "plt.xlabel('time (min.)')\n",
    "plt.ylabel('normalized fluorescence (measure of expression)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relational Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary module\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Create engine: engine\n",
    "engine = create_engine('sqlite:///Chinook.sqlite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the function create_engine from the module sqlalchemy.\n",
    "\n",
    "Create an engine to connect to the SQLite database 'Chinook.sqlite' and assign it to engine.\n",
    "\n",
    "Using the method table_names() on the engine engine, assign the table names of 'Chinook.sqlite' to the variable table_names.\n",
    "\n",
    "Print the object table_names to the shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary module\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Create engine: engine\n",
    "engine = create_engine('sqlite:///Chinook.sqlite')\n",
    "\n",
    "# Save the table names to a list: table_names\n",
    "table_names = engine.table_names()\n",
    "\n",
    "# Print the table names to the shell\n",
    "print(table_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the engine connection as con using the method connect() on the engine.\n",
    "\n",
    "Execute the query that selects ALL columns from the Album table. Store the results in rs.\n",
    "\n",
    "Store all of your query results in the DataFrame df by applying the fetchall() method to the results rs.\n",
    "\n",
    "Close the connection!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "# Create engine: engine\n",
    "engine = create_engine('sqlite:///Chinook.sqlite')\n",
    "\n",
    "# Open engine connection: con\n",
    "con = engine.connect()\n",
    "\n",
    "# Perform query: rs\n",
    "rs = con.execute(\"select * from Album\")\n",
    "\n",
    "# Save results of the query to DataFrame: df\n",
    "df = pd.DataFrame(rs.fetchall())\n",
    "\n",
    "# Close connection\n",
    "con.close()\n",
    "\n",
    "# Print head of DataFrame df\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the SQL query that selects the columns LastName and Title from the Employee table. Store the results in the variable rs.\n",
    "\n",
    "Apply the method fetchmany() to rs in order to retrieve 3 of the records. Store them in the DataFrame df.\n",
    "\n",
    "Using the rs object, set the DataFrame's column names to the corresponding names of the table columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open engine in context manager\n",
    "# Perform query and save results to DataFrame: df\n",
    "with engine.connect() as con:\n",
    "    rs = con.execute(\"select LastName, Title from Employee\")\n",
    "    df = pd.DataFrame(rs.fetchmany(3))\n",
    "    df.columns = rs.keys()\n",
    "\n",
    "# Print the length of the DataFrame df\n",
    "print(len(df))\n",
    "\n",
    "# Print the head of the DataFrame df\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Complete the argument of create_engine() so that the engine for the SQLite database 'Chinook.sqlite' is created.\n",
    "Execute the query that selects all records from the Employee table where 'EmployeeId' is greater than or equal to 6. Use the >= operator and assign the results to rs.\n",
    "Apply the method fetchall() to rs in order to fetch all records in rs. Store them in the DataFrame df.\n",
    "Using the rs object, set the DataFrame's column names to the corresponding names of the table columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create engine: engine\n",
    "engine = create_engine(\"sqlite:///Chinook.sqlite\")\n",
    "\n",
    "# Open engine in context manager\n",
    "# Perform query and save results to DataFrame: df\n",
    "with engine.connect() as con:\n",
    "    rs = con.execute(\"select * from Employee where EmployeeId >= 6\")\n",
    "    df = pd.DataFrame(rs.fetchall())\n",
    "    df.columns = rs.keys()\n",
    "\n",
    "# Print the head of the DataFrame df\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the pandas function read_sql_query() to assign to the variable df the DataFrame of results from the following query: select all records from the table Album.\n",
    "\n",
    "The remainder of the code is included to confirm that the DataFrame created by this method is equal to that created by the previous method that you learned.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "# Create engine: engine\n",
    "engine = create_engine(\"sqlite:///Chinook.sqlite\")\n",
    "\n",
    "# Execute query and store records in DataFrame: df\n",
    "df = pd.read_sql_query(\"select * from Album\", engine)\n",
    "\n",
    "# Print head of DataFrame\n",
    "print(df.head())\n",
    "\n",
    "# Open engine in context manager and store query result in df1\n",
    "with engine.connect() as con:\n",
    "    rs = con.execute(\"SELECT * FROM Album\")\n",
    "    df1 = pd.DataFrame(rs.fetchall())\n",
    "    df1.columns = rs.keys()\n",
    "\n",
    "# Confirm that both methods yield the same result\n",
    "print(df.equals(df1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign to rs the results from the following query: select all the records, extracting the Title of the record and \n",
    "\n",
    "Name of the artist of each record from the Album table and the Artist table, respectively. To do so, INNER JOIN \n",
    "these two tables on the ArtistID column of both.\n",
    "\n",
    "In a call to pd.DataFrame(), apply the method fetchall() to rs in order to fetch all records in rs. Store them in the DataFrame df.\n",
    "\n",
    "Set the DataFrame's column names to the corresponding names of the table columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open engine in context manager\n",
    "# Perform query and save results to DataFrame: df\n",
    "with engine.connect() as con:\n",
    "    rs = con.execute(\"select Title, Name from Album INNER JOIN Artist on Album.ArtistID = Artist.ArtistID\")\n",
    "    df = pd.DataFrame(rs.fetchall())\n",
    "    df.columns = rs.keys()\n",
    "\n",
    "# Print head of DataFrame df\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the pandas function read_sql_query() to assign to the variable df the DataFrame of results from the following query:\n",
    "\n",
    "select all records from PlaylistTrack INNER JOIN Track on PlaylistTrack.TrackId = Track.TrackId that satisfy the condition Milliseconds < 250000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute query and store records in DataFrame: df\n",
    "df = pd.read_sql_query(\"select * from PlaylistTrack INNER JOIN Track on PlaylistTrack.TrackId = Track.TrackId where Milliseconds < 250000\", engine)\n",
    "\n",
    "# Print head of DataFrame\n",
    "print(df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
