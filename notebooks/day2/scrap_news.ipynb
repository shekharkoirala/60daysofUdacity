{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "scrap_news.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shekharkoirala/60daysofUdacity/blob/master/notebooks/day2/scrap_news.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaUtBdxsRRyZ",
        "colab_type": "text"
      },
      "source": [
        "# This Notebook includes scrapping process , finding images , summary , keywords, from news sites. Since, these websites are different in structure, two different class parser are created. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gc7J3P0_RRyc",
        "colab_type": "text"
      },
      "source": [
        "#### !pip3 install newspaper3k #similarly, we could install other libraries if needed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlMwvmJJRRye",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "from datetime import datetime\n",
        "from requests import get\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from newspaper import Article\n",
        "\n",
        "import unicodedata\n",
        "from gensim.summarization.summarizer import summarize\n",
        "from gensim.summarization import keywords"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x042fc2sRRyk",
        "colab_type": "text"
      },
      "source": [
        "## for https://www.nsenergybusiness.com/power/ , attribute \"depth\" calculate how more pages we will scrap."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7vUj0YiRRyl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ParseyA():\n",
        "    def __init__(self, url):\n",
        "        self.url = None\n",
        "        self.u_url = url\n",
        "        self.depth = 100\n",
        "        self.df = pd.DataFrame(columns=['link', 'tags', 'title'])\n",
        "        self.html_soup = None\n",
        "        self.g_article = None\n",
        "        self.error_data = None\n",
        "        self.use_error_data = 0\n",
        "        self.time_ = 0\n",
        "    \n",
        "    def create_soup(self):\n",
        "        \"\"\"\n",
        "        load webpage in beautiful soup\n",
        "        \"\"\"\n",
        "        response = get(self.url)\n",
        "        html_soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        return html_soup\n",
        "    \n",
        "    def create_g_article(self):\n",
        "        \"\"\"\n",
        "        send list of beautiful tags , grid articles\n",
        "        \"\"\"\n",
        "        return self.html_soup.findAll(\"div\",attrs={\"class\":\"grid-x\",\"id\":\"mian-article\"})\n",
        "    \n",
        "    def print_soup(self):\n",
        "        print(self.html_soup)\n",
        "        \n",
        "    def locate_error_data(self):\n",
        "        \"\"\"\n",
        "        fix naming of classes\n",
        "        \n",
        "        fetch lost tag of selected news.\n",
        "        \"\"\"\n",
        "        error_tag = self.html_soup.findAll(\"p\",attrs={\"class\":\"tags project-tag\"})\n",
        "        return {str(idx):[span_tag.text for idx, span_tag in enumerate(error_entries.find_all('span', recursive=False))]\n",
        "                for idx, error_entries in enumerate(error_tag)}\n",
        "\n",
        "    def get_text(self,b_tag):\n",
        "        \"\"\"\n",
        "        b_tag : beautiful soup tag format\n",
        "        Returns\n",
        "        text =  Output_text , string format\n",
        "        \"\"\"\n",
        "        first_search = b_tag.find(\"a\").text\n",
        "        if not first_search:\n",
        "            return b_tag.find(\"img\")['alt']\n",
        "        return first_search\n",
        "\n",
        "    def get_tags(self,b_tag):\n",
        "        \"\"\"\n",
        "        b_tag : beautiful soup tag format\n",
        "        Returns\n",
        "        text =  tags in webpage , list format\n",
        "        \"\"\"\n",
        "        first_search = b_tag.findAll('p', class_=\"tags\")\n",
        "        if first_search:\n",
        "            return [tag.text for tag in first_search[0].findAll('span', recursive=False)]\n",
        "        else:\n",
        "            tag = self.error_data.get(str(self.use_error_data), [])\n",
        "            self.use_error_data += 1\n",
        "            return tag\n",
        "        return []\n",
        "\n",
        "    def get_dataFrame(self):\n",
        "        \"\"\"Dataframe for a scarped webpage\"\"\"\n",
        "        data = [{\"link\":article.select('div[class*=\"cell \"]')[0].find(\"a\")['href'],\n",
        "                  \"title\":self.get_text(article.select('div[class*=\"cell \"]')[0]),\n",
        "                  \"tags\":self.get_tags(article)}\n",
        "                  for article in self.g_article]\n",
        "        return pd.DataFrame(data)\n",
        "    \n",
        "    def show_depth(self):\n",
        "        \"\"\" show how many pages of data we can scrap\"\"\"\n",
        "        strings = html_soup.select('div[class*=\"nav-\"]')[0].findAll('a', class_=\"page-numbers\")[-1]['href'].split('/')\n",
        "        return [x for x in strings if x][-1]\n",
        "    \n",
        "    def parse(self):\n",
        "        for i in range(1, self.depth):\n",
        "            self.url = self.u_url+ \"page/\"+ str(i) + \"/\"\n",
        "            self.html_soup = self.create_soup()\n",
        "            self.g_article = self.create_g_article()\n",
        "            self.error_data = self.locate_error_data()\n",
        "            self.use_error_data = 0\n",
        "            temp_df = self.get_dataFrame()\n",
        "            self.df = pd.concat([self.df,temp_df],ignore_index=True)\n",
        "            \n",
        "    def get_data(self):\n",
        "        \"return the dataframe\"\n",
        "        return self.df\n",
        "    \n",
        "    def extract_post(self, article_url):\n",
        "        \"\"\"Using Newspaper3k library , get insights\n",
        "        Scrapping is used to get links of news, where as all text of news \n",
        "        are scrapped using newspaper3k\n",
        "        \"\"\"\n",
        "        time.sleep(.900)\n",
        "        article = Article(self.format_url(article_url))\n",
        "        article.download()\n",
        "        article.parse()\n",
        "        article.nlp()\n",
        "        if not article.publish_date:\n",
        "            date_ = self.format_date(article)\n",
        "        else:\n",
        "            date_ = article.publish_date\n",
        "        return article.top_image, article.text, article.summary, article.keywords, date_\n",
        "    \n",
        "    def format_url(self,url):\n",
        "        if url[-1] == \"/\":\n",
        "            return url[:-1]\n",
        "        return url\n",
        "    \n",
        "    def format_date(self,article):\n",
        "        _soup = BeautifulSoup(article.html, 'html.parser')\n",
        "        if _soup.find(\"span\", id=\"date\"):\n",
        "            return datetime.strptime(_soup.find(\"span\", id=\"date\").text.strip(), '%d %b %Y')\n",
        "        return ''\n",
        "    \n",
        "    def format_df(self):\n",
        "        \" combining scrapping and newspaper3k insights\"\n",
        "        data= [self.extract_post(row['link']) for idx, row in self.df.iterrows()]\n",
        "        self.df = pd.concat([self.df,pd.DataFrame(data, columns= ['page_image', \"text\", \"summary\", \"keyword\", \"date\"])], axis=1)\n",
        "        self.df['date'] = self.df['date'].fillna(method='bfill')\n",
        "        return self.df\n",
        "    \n",
        "    def extract_post_raw(self, article_url):\n",
        "        \"\"\"without Using Newspaper3k library , get insights\n",
        "        Scrapping is used to get links of news, where as all text of news \n",
        "        are scrapped using raw beautiful soup and requests module\n",
        "        \"\"\"\n",
        "        response = get(self.format_url(article_url))\n",
        "        _soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        if not _soup.findAll('img', class_=\"img_caption\"):\n",
        "            image = \"\"\n",
        "        else:\n",
        "            image = _soup.findAll('img', class_=\"img_caption\")[0]['src']\n",
        "        \n",
        "        table = _soup.findAll('div',attrs={\"class\":\"cell small-12 medium-12 large-10\"})\n",
        "        if not table:\n",
        "            data = \"\"\n",
        "            summary = \"\"\n",
        "            if _soup.findAll('div',attrs={\"class\":\"cell large-10\"}):\n",
        "                table = _soup.findAll('div',attrs={\"class\":\"cell large-10\"})\n",
        "                data = unicodedata.normalize(\"NFKD\", \"\".join([y.text for x in table for y in x.findAll('p')]))\n",
        "                summary = summarize(data)\n",
        "        else:\n",
        "            data = unicodedata.normalize(\"NFKD\", \"\".join([y.text for x in table for y in x.findAll('p')]))\n",
        "            summary = summarize(data)\n",
        "        \n",
        "        if not _soup.find(\"span\", id=\"date\"):\n",
        "            date_ = \"\"\n",
        "        else:\n",
        "            date_ = datetime.strptime(_soup.find(\"span\", id=\"date\").text.strip(), '%d %b %Y')\n",
        "\n",
        "        return image, data, summary, date_\n",
        "\n",
        "    def format_df_raw(self):\n",
        "        \" combining scrapping without using newspaper3k insights\"\n",
        "        data= [self.extract_post_raw(row['link']) for idx, row in self.df.iterrows()]\n",
        "        self.df = pd.concat([self.df,pd.DataFrame(data, columns= ['page_image', \"text\", \"summary\", \"date\"])], axis=1)\n",
        "        self.df['date'] = self.df['date'].fillna(method='bfill')\n",
        "        return self.df\n",
        "\n",
        "parserA = ParseyA(\"https://www.nsenergybusiness.com/power/\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InjZFegJRRyp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "parserA.parse()\n",
        "# df1 = parserA.format_df() # this work pretty well , but some glitches.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tP-LT0zdRRyt",
        "colab_type": "text"
      },
      "source": [
        "# Due to local internet connectivity, or the website service. Sometimes the data scraped got multiple redirects scrapping in Google Collab looks a feasible smart idea. https://colab.research.google.com/drive/1uGUbH9YIjzLyCT4j2uGpAgYt9yp9QZpg"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMBJd_M3RRyu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df1 = parserA.format_df_raw()  # raw method, without using newspaper3k, with faster result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0wUvFeiRRzG",
        "colab_type": "text"
      },
      "source": [
        "# for . https://www.esi-africa.com/category/news/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgNknFg3RRzI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ParseyB():\n",
        "    def __init__(self, url):\n",
        "        self.url = None\n",
        "        self.u_url = url\n",
        "        self.depth = 100     # change for how much news page we want to scrap\n",
        "        self.df = pd.DataFrame(columns=['link', 'image', 'title'])\n",
        "        self.html_soup = None\n",
        "        self.g_article = None\n",
        "    \n",
        "    def create_soup(self):\n",
        "        \"\"\"\n",
        "        load webpage in beautiful soup\n",
        "        \"\"\"\n",
        "        headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'}\n",
        "        result = get(self.url, headers=headers)\n",
        "        html_soup = BeautifulSoup(result.text, 'html.parser')\n",
        "        return html_soup\n",
        "    \n",
        "    def create_g_article(self):\n",
        "        \"\"\"\n",
        "        send list of beautiful tags , grid articles\n",
        "        \"\"\"\n",
        "        return self.html_soup.findAll('div', class_=\"td-module-thumb\")\n",
        "    \n",
        "    def print_soup(self):\n",
        "        print(self.html_soup)\n",
        "\n",
        "    def get_dataFrame(self):\n",
        "        data = [{\"image\" : article.findAll('img')[0]['src'],\n",
        "                  \"link\"  : article.find('a')['href'],\n",
        "                  \"title\" : article.find('a')['title']}for article in self.g_article]\n",
        "        return pd.DataFrame(data)\n",
        "    \n",
        "    def show_depth(self):\n",
        "        \"\"\" show how many pages of data we can scrap\"\"\"\n",
        "        strings = html_soup.select('div[class*=\"nav\"]')[0].findAll('a', class_=\"last\")[0]['href'].split(\"/\")\n",
        "        return [x for x in strings if x][-1]\n",
        "    \n",
        "    def parse(self):\n",
        "        for i in range(1, self.depth):\n",
        "            self.url = self.u_url+ \"page/\"+ str(i) + \"/\"\n",
        "            self.html_soup = self.create_soup()\n",
        "            self.g_article = self.create_g_article()\n",
        "            temp_df = self.get_dataFrame()\n",
        "            self.df = pd.concat([self.df,temp_df],ignore_index=True)\n",
        "            \n",
        "    def get_data(self):\n",
        "        return self.df\n",
        "    \n",
        "    def extract_post(self, article_url):\n",
        "        \"\"\"Using Newspaper3k library , get insights\n",
        "        Scrapping is used to get links of news, where as all text of news \n",
        "        are scrapped using newspaper3k\n",
        "        \"\"\"\n",
        "        time.sleep(.500)\n",
        "        article = Article(article_url)\n",
        "        article.download()\n",
        "        article.parse()\n",
        "        article.nlp()\n",
        "        return article.top_image, article.text, article.summary, article.keywords, article.publish_date\n",
        "    \n",
        "    def format_df(self):\n",
        "        \" combining scrapping and newspaper3k insights\"\n",
        "        data= [self.extract_post(row['link']) for idx, row in self.df.iterrows()]\n",
        "        self.df = pd.concat([self.df,pd.DataFrame(data, columns= ['page_image', \"text\", \"summary\", \"keyword\", \"date\"])], axis=1)\n",
        "        return self.df\n",
        "        \n",
        "parserB = ParseyB(\"https://www.esi-africa.com/category/news/\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsFngXifRRzN",
        "colab_type": "code",
        "colab": {},
        "outputId": "de7b365b-ebee-4149-f1b5-d256ae954482"
      },
      "source": [
        "t = time.process_time()\n",
        "parserB.parse()\n",
        "elapsed_time = time.process_time() - t\n",
        "print(\"Time to parse : \", elapsed_time,  \"sec\")\n",
        "t = time.process_time()\n",
        "df2 = parserB.format_df()\n",
        "elapsed_time = time.process_time() - t\n",
        "print(\"Time to get dataframe : \", elapsed_time,  \"sec\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/home/dipesh/.local/lib/python3.6/site-packages/ipykernel_launcher.py:45: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
            "of pandas will change to not sort by default.\n",
            "\n",
            "To accept the future behavior, pass 'sort=False'.\n",
            "\n",
            "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Time to parse :  7.067197495999949 sec\n",
            "Time to get dataframe :  266.07974973999995 sec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIw1jiqkRRzT",
        "colab_type": "text"
      },
      "source": [
        "# Two csv are generated from this Notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8QHtra1RRzU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df2.to_csv(\"esiafrica_1980.csv\", encoding='utf-8', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IYTs-2IRRzX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df1.to_csv(\"nsenergy_990.csv\", encoding='utf-8', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1AggQ7ZRRzb",
        "colab_type": "code",
        "colab": {},
        "outputId": "0f235694-ee45-40fc-c38a-3047964f972e"
      },
      "source": [
        "df1.head(2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>link</th>\n",
              "      <th>tags</th>\n",
              "      <th>title</th>\n",
              "      <th>page_image</th>\n",
              "      <th>text</th>\n",
              "      <th>summary</th>\n",
              "      <th>date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://www.nsenergybusiness.com/news/sharp-bu...</td>\n",
              "      <td>[Power, Solar, PV]</td>\n",
              "      <td>Sharp builds solar plant near New Ulaanbaatar ...</td>\n",
              "      <td>https://www.nsenergybusiness.com/wp-content/up...</td>\n",
              "      <td>Sharp Energy Solutions Corporation (SESJ) comp...</td>\n",
              "      <td>Sharp Energy Solutions Corporation (SESJ) comp...</td>\n",
              "      <td>2019-06-14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://www.nsenergybusiness.com/news/jolywood...</td>\n",
              "      <td>[Power, Solar, PV]</td>\n",
              "      <td>Jolywood signs agreement to supplyBifacial Mod...</td>\n",
              "      <td>https://www.nsenergybusiness.com/wp-content/up...</td>\n",
              "      <td>Jolywood, a Chinese manufacturer of bifacial s...</td>\n",
              "      <td>Jolywood, a Chinese manufacturer of bifacial s...</td>\n",
              "      <td>2019-06-14</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                link                tags  \\\n",
              "0  https://www.nsenergybusiness.com/news/sharp-bu...  [Power, Solar, PV]   \n",
              "1  https://www.nsenergybusiness.com/news/jolywood...  [Power, Solar, PV]   \n",
              "\n",
              "                                               title  \\\n",
              "0  Sharp builds solar plant near New Ulaanbaatar ...   \n",
              "1  Jolywood signs agreement to supplyBifacial Mod...   \n",
              "\n",
              "                                          page_image  \\\n",
              "0  https://www.nsenergybusiness.com/wp-content/up...   \n",
              "1  https://www.nsenergybusiness.com/wp-content/up...   \n",
              "\n",
              "                                                text  \\\n",
              "0  Sharp Energy Solutions Corporation (SESJ) comp...   \n",
              "1  Jolywood, a Chinese manufacturer of bifacial s...   \n",
              "\n",
              "                                             summary       date  \n",
              "0  Sharp Energy Solutions Corporation (SESJ) comp... 2019-06-14  \n",
              "1  Jolywood, a Chinese manufacturer of bifacial s... 2019-06-14  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    }
  ]
}